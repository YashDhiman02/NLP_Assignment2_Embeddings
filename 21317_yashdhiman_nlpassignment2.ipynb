{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/YashDhiman02/NLP_Assignment2_Embeddings/blob/main/21317_yashdhiman_nlpassignment2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I25cpE_9Spur",
        "outputId": "e55d4113-95e9-493c-b250-2a61eb30af01"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.3840 - loss: 0.8535\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 34ms/step\n",
            "Average Macro F1 Score: 0.2892302258191323\n"
          ]
        }
      ],
      "source": [
        "# Below are 4 programs, first three of which are separated by ''' ''' comments.\n",
        "# First Commented out code learns embeddings from the training data\n",
        "# Second Commented out code runs FFNN model\n",
        "# Third Commented out code tuns LSTM model\n",
        "# Fourth code runs the RNN model(it gives best F1 score).\n",
        "\n",
        "'''\n",
        "#Training embeddings\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import gensim\n",
        "from gensim.models import Word2Vec\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "train_url = 'https://raw.githubusercontent.com/debajyotimaz/nlp_assignment/main/train_split.csv'\n",
        "data = pd.read_csv(train_url)\n",
        "\n",
        "sentences = data['text'].astype(str).tolist()\n",
        "\n",
        "tokenized_sentences = [gensim.utils.simple_preprocess(sentence) for sentence in sentences]\n",
        "\n",
        "embedding_size = 100  # Set less than 100 as required\n",
        "word2vec_model = Word2Vec(sentences=tokenized_sentences, vector_size=embedding_size, window=5, min_count=1, workers=4)\n",
        "\n",
        "word2vec_model.save(\"custom_word2vec.model\")\n",
        "\n",
        "word2vec_model.wv.save_word2vec_format(\"custom_word2vec_embeddings.txt\", binary=False)\n",
        "\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(sentences)\n",
        "sequences = tokenizer.texts_to_sequences(sentences)\n",
        "\n",
        "max_length = max([len(seq) for seq in sequences])\n",
        "padded_sequences = pad_sequences(sequences, maxlen=max_length, padding='post')\n",
        "\n",
        "vocab_size = len(tokenizer.word_index) + 1  # +1 because of reserved index 0\n",
        "embedding_matrix = np.zeros((vocab_size, embedding_size))\n",
        "\n",
        "for word, i in tokenizer.word_index.items():\n",
        "    if word in word2vec_model.wv:\n",
        "        embedding_matrix[i] = word2vec_model.wv[word]\n",
        "\n",
        "print(\"Word2Vec model trained and embeddings saved.\")\n",
        "'''\n",
        "\n",
        "\n",
        "'''\n",
        "#FFNN\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, Dense, Flatten, Dropout\n",
        "from tensorflow.keras.optimizers import Adam, AdamW, SGD\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import f1_score\n",
        "from gensim.models import Word2Vec\n",
        "\n",
        "embedding_url = 'https://raw.githubusercontent.com/YashDhiman02/NLP_Assignment2_Embeddings/main/custom_word2vec.model'\n",
        "word2vec_model = Word2Vec.load(embedding_url)\n",
        "\n",
        "train_url = 'https://raw.githubusercontent.com/debajyotimaz/nlp_assignment/main/train_split.csv'\n",
        "train_data = pd.read_csv(train_url)\n",
        "\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "MAX_SEQUENCE_LENGTH = 80\n",
        "MAX_NB_WORDS = 20000\n",
        "EMBEDDING_DIM = word2vec_model.vector_size\n",
        "\n",
        "# Tokenize the text data\n",
        "tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n",
        "tokenizer.fit_on_texts(train_data['text'])\n",
        "X = tokenizer.texts_to_sequences(train_data['text'])\n",
        "\n",
        "# Pad the sequences to a fixed length (128 tokens)\n",
        "X_padded = pad_sequences(X, maxlen=MAX_SEQUENCE_LENGTH)\n",
        "\n",
        "# Extract labels (emotion columns: Joy, Fear, Anger, Sadness, Surprise)\n",
        "y = train_data[['Joy', 'Fear', 'Anger', 'Sadness', 'Surprise']].values\n",
        "\n",
        "# Split the data into training and validation sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_padded, y, test_size=0.2, random_state=42)\n",
        "\n",
        "VOCAB_SIZE = min(MAX_NB_WORDS, len(tokenizer.word_index) + 1)\n",
        "embedding_matrix = np.zeros((VOCAB_SIZE, EMBEDDING_DIM))\n",
        "\n",
        "for word, i in tokenizer.word_index.items():\n",
        "    if i < VOCAB_SIZE and word in word2vec_model.wv:\n",
        "        embedding_matrix[i] = word2vec_model.wv[word]\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "model.add(Embedding(input_dim=VOCAB_SIZE,\n",
        "                    output_dim=EMBEDDING_DIM,\n",
        "                    weights=[embedding_matrix],\n",
        "                    input_length=MAX_SEQUENCE_LENGTH,\n",
        "                    trainable=False))  # Set trainable=False to freeze the Word2Vec embeddings\n",
        "\n",
        "model.add(Flatten())\n",
        "\n",
        "model.add(Dense(50, activation='relu'))  # First hidden layer\n",
        "model.add(Dense(50, activation='relu'))  # Second hidden layer\n",
        "\n",
        "model.add(Dense(5, activation='sigmoid'))  # Sigmoid for multi-label classification\n",
        "\n",
        "learning_rate = 0.001\n",
        "optimizer = AdamW(learning_rate=learning_rate)  # Choose the optimizer (AdamW used here)\n",
        "\n",
        "model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "history = model.fit(X_train, y_train, epochs=60, batch_size=20, validation_data=(X_val, y_val), verbose=0)\n",
        "\n",
        "val_loss, val_accuracy = model.evaluate(X_val, y_val, batch_size=20,verbose=0)\n",
        "#print(f'Validation Loss: {val_loss}')\n",
        "#print(f'Validation Accuracy: {val_accuracy}')\n",
        "\n",
        "test_url = 'https://raw.githubusercontent.com/debajyotimaz/nlp_assignment/main/test_split.csv'\n",
        "test_data = pd.read_csv(test_url)\n",
        "\n",
        "X_test = tokenizer.texts_to_sequences(test_data['text'])\n",
        "X_test_padded = pad_sequences(X_test, maxlen=MAX_SEQUENCE_LENGTH)\n",
        "\n",
        "y_test = test_data[['Joy', 'Fear', 'Anger', 'Sadness', 'Surprise']].values\n",
        "\n",
        "y_pred = model.predict(X_test_padded)\n",
        "\n",
        "y_pred_bin = (y_pred > 0.5).astype(int)\n",
        "\n",
        "f1 = f1_score(y_test, y_pred_bin, average='macro')\n",
        "\n",
        "print(f'Average Macro F1 Score: {f1}')\n",
        "\n",
        "'''\n",
        "\n",
        "\n",
        "\n",
        "'''\n",
        "#LSTM\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Flatten\n",
        "from tensorflow.keras.optimizers import Adam, AdamW, SGD\n",
        "from gensim.models import Word2Vec\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "train_url = 'https://raw.githubusercontent.com/debajyotimaz/nlp_assignment/main/train_split.csv'\n",
        "train_data = pd.read_csv(train_url)\n",
        "\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "MAX_SEQUENCE_LENGTH = 128  # Maximum sequence length (tokens)\n",
        "MAX_NB_WORDS = 10000  # Maximum number of words in the vocabulary (you can adjust)\n",
        "EMBEDDING_DIM = 100  # Size of embedding vectors (could also be trained)\n",
        "VOCAB_SIZE = MAX_NB_WORDS\n",
        "\n",
        "tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n",
        "tokenizer.fit_on_texts(train_data['text'])\n",
        "X = tokenizer.texts_to_sequences(train_data['text'])\n",
        "\n",
        "X_padded = pad_sequences(X, maxlen=MAX_SEQUENCE_LENGTH)\n",
        "\n",
        "y = train_data[['Joy', 'Fear', 'Anger', 'Sadness', 'Surprise']].values\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_padded, y, test_size=0.2, random_state=42)\n",
        "\n",
        "embedding_url = 'https://raw.githubusercontent.com/YashDhiman02/NLP_Assignment2_Embeddings/main/custom_word2vec.model'\n",
        "word2vec_model = Word2Vec.load(embedding_url)\n",
        "\n",
        "embedding_matrix = np.zeros((VOCAB_SIZE, EMBEDDING_DIM))\n",
        "word_index = tokenizer.word_index\n",
        "\n",
        "for word, i in word_index.items():\n",
        "    if i < VOCAB_SIZE and word in word2vec_model.wv:\n",
        "        embedding_matrix[i] = word2vec_model.wv[word]\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "model.add(Embedding(input_dim=VOCAB_SIZE,\n",
        "                    output_dim=EMBEDDING_DIM,\n",
        "                    weights=[embedding_matrix],\n",
        "                    input_length=MAX_SEQUENCE_LENGTH,\n",
        "                    trainable=False))\n",
        "\n",
        "model.add(LSTM(64, activation='tanh', return_sequences=True))\n",
        "model.add(LSTM(64, activation='tanh'))\n",
        "model.add(Dense(64, activation='relu'))\n",
        "\n",
        "model.add(Dense(5, activation='sigmoid'))  # Sigmoid for multi-label classification\n",
        "\n",
        "learning_rate = 0.001\n",
        "optimizer = AdamW(learning_rate=learning_rate)\n",
        "\n",
        "model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "history = model.fit(X_train, y_train, epochs=40, batch_size=20, validation_data=(X_val, y_val),verbose=0)\n",
        "\n",
        "\n",
        "val_loss, val_accuracy = model.evaluate(X_val, y_val, batch_size=20)\n",
        "#print(f'Validation Loss: {val_loss}')\n",
        "#print(f'Validation Accuracy: {val_accuracy}')\n",
        "\n",
        "test_url = 'https://raw.githubusercontent.com/debajyotimaz/nlp_assignment/main/test_split.csv'\n",
        "test_data = pd.read_csv(test_url)\n",
        "\n",
        "X_test = tokenizer.texts_to_sequences(test_data['text'])\n",
        "X_test_padded = pad_sequences(X_test, maxlen=MAX_SEQUENCE_LENGTH)\n",
        "\n",
        "y_test = test_data[['Joy', 'Fear', 'Anger', 'Sadness', 'Surprise']].values\n",
        "\n",
        "y_pred = model.predict(X_test_padded)\n",
        "\n",
        "y_pred_bin = (y_pred > 0.5).astype(int)\n",
        "\n",
        "f1 = f1_score(y_test, y_pred_bin, average='macro')\n",
        "print(f'Average Macro F1 Score: {f1}')\n",
        "'''\n",
        "\n",
        "#RNN\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, SimpleRNN, Dense, Dropout\n",
        "from tensorflow.keras.optimizers import Adam, AdamW, SGD\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import f1_score\n",
        "from gensim.models import Word2Vec\n",
        "\n",
        "import random\n",
        "import os\n",
        "np.random.seed(42)\n",
        "random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "os.environ['TF_DETERMINISTIC_OPS'] = '1'\n",
        "\n",
        "embedding_url = 'https://raw.githubusercontent.com/YashDhiman02/NLP_Assignment2_Embeddings/main/custom_word2vec.model'\n",
        "word2vec_model = Word2Vec.load(embedding_url)\n",
        "\n",
        "train_url = 'https://raw.githubusercontent.com/debajyotimaz/nlp_assignment/main/train_split.csv'\n",
        "train_data = pd.read_csv(train_url)\n",
        "\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "MAX_SEQUENCE_LENGTH = 124\n",
        "MAX_NB_WORDS = 10000\n",
        "EMBEDDING_DIM = word2vec_model.vector_size\n",
        "\n",
        "tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n",
        "tokenizer.fit_on_texts(train_data['text'])\n",
        "X = tokenizer.texts_to_sequences(train_data['text'])\n",
        "\n",
        "X_padded = pad_sequences(X, maxlen=MAX_SEQUENCE_LENGTH)\n",
        "\n",
        "y = train_data[['Joy', 'Fear', 'Anger', 'Sadness', 'Surprise']].values\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_padded, y, test_size=0.2, random_state=42)\n",
        "\n",
        "VOCAB_SIZE = min(MAX_NB_WORDS, len(tokenizer.word_index) + 1)\n",
        "embedding_matrix = np.zeros((VOCAB_SIZE, EMBEDDING_DIM))\n",
        "\n",
        "for word, i in tokenizer.word_index.items():\n",
        "    if i < VOCAB_SIZE and word in word2vec_model.wv:\n",
        "        embedding_matrix[i] = word2vec_model.wv[word]\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "model.add(Embedding(input_dim=VOCAB_SIZE,\n",
        "                    output_dim=EMBEDDING_DIM,\n",
        "                    weights=[embedding_matrix],\n",
        "                    input_length=MAX_SEQUENCE_LENGTH,\n",
        "                    trainable=False))\n",
        "\n",
        "model.add(SimpleRNN(64, return_sequences=True, activation='relu'))\n",
        "model.add(SimpleRNN(64, activation='relu'))\n",
        "\n",
        "model.add(Dense(5, activation='sigmoid'))\n",
        "\n",
        "learning_rate = 0.001\n",
        "optimizer = AdamW(learning_rate=learning_rate)\n",
        "\n",
        "model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "history = model.fit(X_train, y_train, epochs=40, batch_size=20, validation_data=(X_val, y_val), verbose=0)\n",
        "\n",
        "\n",
        "val_loss, val_accuracy = model.evaluate(X_val, y_val, batch_size=20)\n",
        "#print(f'Validation Loss: {val_loss}')\n",
        "#print(f'Validation Accuracy: {val_accuracy}')\n",
        "\n",
        "test_url = 'https://raw.githubusercontent.com/debajyotimaz/nlp_assignment/main/test_split.csv'\n",
        "test_data = pd.read_csv(test_url)\n",
        "\n",
        "X_test = tokenizer.texts_to_sequences(test_data['text'])\n",
        "X_test_padded = pad_sequences(X_test, maxlen=MAX_SEQUENCE_LENGTH)\n",
        "\n",
        "y_test = test_data[['Joy', 'Fear', 'Anger', 'Sadness', 'Surprise']].values\n",
        "y_pred = model.predict(X_test_padded)\n",
        "\n",
        "y_pred_bin = (y_pred > 0.5).astype(int)\n",
        "\n",
        "f1 = f1_score(y_test, y_pred_bin, average='macro')\n",
        "print(f'Average Macro F1 Score: {f1}')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNAHLdEWYeVTGeiHW6PdQ+q",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}